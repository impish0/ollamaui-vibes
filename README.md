# Ollama UI Vibes ğŸš€

A beautiful, fast, and feature-rich web interface for interacting with local Ollama models.

## Features

- ğŸ¨ **Beautiful Modern UI** - Clean, responsive interface with dark/light mode
- âš¡ **Real-time Streaming** - Smooth streaming responses with markdown rendering
- ğŸ’¬ **Multi-Chat Support** - Create and manage multiple chat conversations
- ğŸ”„ **Side-by-Side Comparison** - Compare responses from different models (coming soon)
- ğŸ“ **System Prompts** - Create, manage, and apply custom system prompts to any chat
- ğŸ¤– **Auto Model Polling** - Automatically detects available Ollama models
- ğŸ“Š **Auto Title Generation** - AI-generated titles for your conversations
- ğŸ’¾ **SQLite Storage** - All data stored locally in SQLite database
- ğŸ”’ **Localhost Security** - Built-in security to prevent unauthorized network access

## Prerequisites

- Node.js 18+ and npm
- [Ollama](https://ollama.ai/) installed and running locally

## Quick Start

1. **Install dependencies:**
   ```bash
   npm install
   ```

2. **Set up the database:**
   ```bash
   npm run db:migrate
   ```

3. **Start the application:**
   ```bash
   npm run dev
   ```

4. **Open your browser:**
   Navigate to [http://localhost:5173](http://localhost:5173)

## Available Scripts

- `npm run dev` - Start both client and server in development mode
- `npm run dev:client` - Start only the Vite dev server
- `npm run dev:server` - Start only the Express API server
- `npm run build` - Build the production bundle
- `npm run db:migrate` - Run database migrations
- `npm run db:generate` - Generate Prisma Client
- `npm run db:studio` - Open Prisma Studio (database GUI)

## Configuration

The application can be configured via environment variables in the `.env` file:

```env
OLLAMA_BASE_URL=http://localhost:11434
SERVER_PORT=3001
NODE_ENV=development
DATABASE_URL="file:./dev.db"
```

## Tech Stack

- **Frontend:** React 18 + TypeScript + Vite
- **Styling:** Tailwind CSS v3
- **Backend:** Express + Node.js
- **Database:** SQLite + Prisma ORM
- **State Management:** Zustand
- **Markdown:** react-markdown with syntax highlighting
- **AI:** Ollama (local models)

## Features in Detail

### Chat Management
- Create new chats with any available model
- View all your chat history in the sidebar
- Delete chats you no longer need
- Auto-generated titles based on conversation content

### System Prompts
- Create reusable system prompts
- Apply different prompts to different chats
- Edit and delete prompts as needed
- Quick-select from dropdown in chat header

### Model Selection
- Automatically polls Ollama for available models every 15 seconds
- Shows model size and parameter info
- Switch models mid-conversation
- Remembers the last model used per chat

### Streaming
- Real-time streaming of AI responses
- Progressive markdown rendering
- Syntax highlighting for code blocks
- Smooth typing indicators

### Security
- Server binds to 127.0.0.1 (localhost only)
- CORS restricted to localhost origins
- Rate limiting on API endpoints
- Input sanitization

## Project Structure

```
/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ client/          # React frontend
â”‚   â”‚   â”œâ”€â”€ components/  # UI components
â”‚   â”‚   â”œâ”€â”€ hooks/       # Custom React hooks
â”‚   â”‚   â”œâ”€â”€ services/    # API services
â”‚   â”‚   â”œâ”€â”€ store/       # Zustand state management
â”‚   â”‚   â””â”€â”€ App.tsx      # Main app component
â”‚   â”œâ”€â”€ server/          # Express backend
â”‚   â”‚   â”œâ”€â”€ routes/      # API routes
â”‚   â”‚   â”œâ”€â”€ services/    # Business logic
â”‚   â”‚   â””â”€â”€ middleware/  # Express middleware
â”‚   â””â”€â”€ shared/          # Shared types
â”œâ”€â”€ prisma/              # Database schema & migrations
â””â”€â”€ public/              # Static assets
```

## Development

The application runs in development mode with:
- Hot module replacement (HMR) for the frontend
- Auto-restart for the backend via tsx watch
- Vite proxy for seamless API calls

## License

MIT

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.
